Slide 1:
========
Introduce yourself and the title


Slide 2: Benefits of Public Cloud for Financial Exchanges
========

Introduce benefits of cloud exchanges i.e., building financial exchanges on the public cloud as opposed to building in private data centers or colocation facilities.

- In colocation facilities, the exchange and market participants need to be connected with equal length cables to meet the latency fairness contraints that we will take a look at soon. But because the exchange and participants need to be connected with equal length cables, the constraints of physical space emerge as all the machines need to be colocated close by. Any system built on the public cloud does not have such restrictions so if we can achieve the correctness/fairness requirements of a financial exchange while building it on public cloud, we would practically have elminated the physical space constraints. 

- And that also gives us the benefit of scaling to a large number of participants as now adding one participant would be adding one more VM in the public cloud which is way easier than adding a colocated participant. 

- The public cloud lowers the barriers to entry for launching new global markets as anyone can rent a VM in the public cloud with much ease and become a participant. 

- The cloud also has a lot analytics or ML services residing nearby, much more compute and storage avaialble nearby which is not very easy to achieve in colocation facilities. 

- And then there are several typical benefits of cloud which include flexible resource allocation, potential reduction in costs and etc.


Mention that there are some cons along with pros. 
- The difference in latency perceived by any participants is minimized to sub-microsecond in on-prem while in the cloud that is extremely hard. we achieve a latency difference of less than a microsecond across all participants. and the absolute values of latencies are also high and variable in the public cloud as opposed to the colocation facilities. 


Slide 3: Onyx: Scalable Cloud Financial Exchange
========

Introduce onyx, while briefly introducing fairness definitions necessary to introduce onyx. 

- Now that we have some perspective on building financial exchanges on the public cloud, we introduce Onyx, a scalable cloud financial exchange. 
- One component of Onyx is a fair multicast service. It disseminates market data to participants. Market data is information about market state i..e., what asset has been sold, what asset is avaiable to be sold and what are the respective prices. Partipants use this information to generate buy or sell orders. The multicast service ensures that all participants, where we support 1000 of them, receive each message within a microsecond of each other so that all participants have a fair chance of trading on any information from the exchange.

- The second component of Onyx is an order submissin service which fairly handle all the orders generated by the market participants. here fairness amounts to executing orders in the order of when they are generated by participants. Moreover, this service is designed to maintain fairness while handling bursty traffic from participants. Some trading events, some market data may lead to a herd of participants generating orders which leads to bursty traffic. Onyx has a mechanism to gracefully handle bursts. 

- We have prototyped Onyx both on AWS and GCP.

Slide 4 to 14: Primer On Financial Exchanges
========

- There is an exchange server and several market participants. The exchange multicasts market data to the participants, which for the sake of Onyx, we can assume is a broadcast. There can be several such data points that are multicasted. 

- The participants may generate orders, perhaps in response to the received data or in response to any other external information. These orders are received by the exchange and processed. Processing of orders essentially means mathcing one order with another. Somebody wants to buy and sombody wants to sell, you match them topgether and orders get executed i.e., trade happens. 

(the slide with LOB snapshot, the figure with multiple price levels)
- Which orders to match with what other orders is decided by some matching algorithm. One such popular algorithm is price-time priority algorithm. Where the exchange maintains a record of all bids and ask orders in a book. Incoming orders may get matched with each other right away or they may be placed in the book if they cannot be matched. Orders are sorted into several price levels where the orders in one price level are sorted based on when they were received. With equal lenght cables, it is equiavalent to sorting based on when they were generated by respective participants. 

- When an order crosses the mid-point (the mid-price) it is very likely to get matched. For example an ask order of $4 will get matched with some bid order in the $4 price level. It will precisely be the bid which was generated earliest among the other $4 bid orders. 

- An order closer to the mid price has higher changes of execution while an order away from the mid-price will be matched when mid-price moves closer to them. 

- Position in each price level matters as it decides whether it gets matched before or after other orders in the same price level. 

- This creates a competiion among the participants and participants try to generate orders before others so they their orders can get matched as well as they try to hold positions in the price levels away from the mid-price for future matching. 

- To enable a fair competition, exchanges are built in private data centers so that latency from all participants to the exchange can be equalized to ahve fair order matching while latency from exchnage to participants is equalized to ahve fair market information dissemination. 

Slide: Public Cloud Exhibits High Latency Variance

- Now that we have a sense of what exchange do and what type of competition happens, lets see why it is difficult to enable that fair compeition on the public cloud. 
- The cloud exhibits high latency vairnace
- latecny changes temporally as well as spatially i.e., across clients. Latency spikes can occur on some links where the latency can increase by almost 10x. Such a networking fabric is unfit for fair competiion
- thats where onyx comes into the pictuyre as it intorduces mechanisms to achieve fairness on top of the best-effort networking fabric of the public cloud while realizing high performance at scale.

Slide: Fairness
========

- With all that context, now lets talk about the types of fairness that exchanges need. There is (i) outbound fairness and (ii) inbound fairness. 

- outbound fairness refers to data going outboudn from  exchnage to the participants. we want the difference in data reception time for any two participatsn to be 0 for every market data message. 

- in inbound fairness, as participants generate messages we want the exchange to process them in the order of their generation. 

- these two definitions are idealitic and perhaps not realizable. What we and, other exchange achieve is an approximation in practice. 

- here is the approximation that onyx achieves. 

- in the outbound, instead of exact 0 time difference we try to achieve as close to 0 as possible and the target we achieve is less than a time difference of <= 1 microsecond (at 90+ percentile, perhaps no need to mention percentile here though to keep it simple). 

- in the inbound, fairness refers to processing orders in the order of generation timestamps where all the clocks of participants are tightly synchronized. 

- We achieve about while being performant at sclae. and we target 1000 participants for achieving high scale, this is as opposed to less than 200 particioants that a typiucal financial exchange supports. 

- each participant is a separate VM in our prototype. 

Slide: Outbound Fairness
========

- Outbound fairness is realized by  a protocol hold-and-release. The exchange adds a deadline to the outgoing messages and the participants hold the messages before processing it. They hold it until the deadline. The deadline is calculated to be sufficiently in future so that all participants would have received the message before the deadline arrives. Assume for this talk that participants are trusted and they follow this protocol. Now this simple mechanism works well for a small number of aprticipants but as we scale the number of participants, latency flucturations increases, the dealdine become less effecitve as some paritciapnts may not receive the messages before the deadline becuase of latency spikes. Or the deadlines need to be set far into the future to account for potential spikes but that increases the overall latency as participants have to hold for a long time. 

- We devise technique to achiev scalable fair multicast by employing a tree that helps dissemnate data to a large number of participants while maintaining low latency. Without a tree, exchange has to serially send messages to all participants and the latency experienced by later participants becomes higher than the latenxy experienced by earlier participants. tree eliminates that pattern. 

- but a tree introduces more nodes in the path of messages and increases the changes of spikes or perofmrance variations at the intermediate nodes impacting the latency of messages. 

- we augment the tree with three techniques aimed at reducing latency variance caused by some compoents of the system.

- latency spikes can occur at the links connecting various nodes of the tree, 
- intermediate tree nodes/proxies can suffer performance varaince and may increase latency of some messages passing through them. 
- or the receiver/participant vms may suffer from performance variation and may take long time to process an incoming message. 

- one tehcnique is introduced to target each of them.

- in round robin packet spraying, the children of each tree node continuously change so that a path between two nodes is not continuously utilized. as some spike occurs on a path, it will not affect several successive messages. 

-  in proxy heding, each tree node has multiple parents intead of just one, i.e., H parent, each sending copies of the message and a child node can process the earliest one and discard any late arriving copies. 

- lastly, we assign two VMs per participant, running the same trading algorithm so that if one VM lags, the other can keep up.

-  we will have to skip any more detail becuase of the time.

Slide: Scalable Inbound Fairness
========

- the order submission service has two main compoents, a sequencer that ensures inbound fairness and a scheduling policy that enahnces performance during bursts while preserving fairness. 

Slide: A sequencer
========

- It guarantees that orders are seen by the exchange serve in the order of the generation timestamps attached by the participants. and it relies on clock synchronization. 

- clients/participants attach generation timestamps to all outgoing messages/orders.
- sequencer waits for atleast one message from every client and it releases the one with the smallest timestamp. it continuously keeps on doing that which leads to achieving inbound fairness

- progress may halt under client failures as one message from each client is needed for progress
- however fairness is guaranteed assuming clock sync. 
- in practice, the clock sync is not perfect but our clock sync is sufficiently higher than the time granularity of interest. 
- we measure time in microseconds at best while the clocks are synchronized at the level of nanoseconds (~100 nanosecond offset among all participants).

Slide: Limit Order Queue
========

- bursts of orders may occur upon some sensitive event in the market
- the exchange may not be able to keep up with the offered load
- LOQ figures out what load/orders to prioritize over other orders so that the exchange is doing the most critical work. 
- LOQ works at the egress of each participant as well as each intermeidate node between the participant and the exchange. (as we utilize the tree in reverse, the proxies are the intermediate nodes)
- LOQ figures out what orders will be matched in near future while what orders will just stay in the limit order book and wait to get matched
- the ones that need to wait, will just wait in the network instead. 

- however, naively doing such prioritization based on an order's price's distance from mid-point may violate inbound fairness.

Slide: Limit Order Queue Gracefully Handles Bursts
========

- before further diving into how LOQ does scheduling while preserving fairness, lets take a quick look at how it improves performance.
- this graph shows Onys (utilizing LOQ) and another system CloudEx. We see the Ony's order matching rate (i.e, processing packets) is higher than Cloudex, especially during shaded regions which represent the bursts of orders. during bursts the order generation rate of participants becomes 20x (i.e., 100K --> 20 * 100K).


- now we will look at how fairness is preserved while employing a scheduling policy

Slide: Limit Order Queue Preserves Fairness
========

- consider a setup where there is an exchange, several participants and they are connected via a FIFO fabric i.e., whoever generates the order first, their order is presented to the exchange first. 

- in such a setup, the orders generated will go through the fabric and appear at the exchange in a sequence, lets call I_FIFO. These orders are processed by the exchange, i.e, some are matched together, some are put into LOB and matched later. After processing all orders (for a certain period of time), the sequence of matched/executed orders is called S_FIFO. 

- Now lets consider a similar setup but the participants and exchange are connected via an LOQ fabric i.e., instead of fifo, orders go through LOQ. 

- in this setup, the orders will appear at the excange in a sequence, I_LOQ and be processed and the output sequence will be called S_LOQ. 

- What we want is S_FIFO be equal to S_LOQ. while I_FIFO may not necessarily be equal to I_LOQ. 

- Having S_FIFO = S_LOQ preserves fairness i.e, the order in which orders are matched/executed it stays the same as having a fifo fabric. 
- but the option to have I_FIFO not necessarily equal to I_LOQ gives us room to do scheduling that can enahnce performance i..e, prioritize some orders over the others, but only as long as it leads to S_FIFO = S_LOQ. 

- Design of LOQ capitalizes on this room of scheduling and improves performance while preserving fairness. the design details are in the paper. 


Evaluation (3 slides):
========

The first result shows multicast latency comparison between Onyx, aws transit gateway based multicast and a DU, direct unicasts approach. In DU, exchange simply sends messages to all participants without utilizing any other technique i.e,, tree or hold-and-release or hedging. The CDF shows how Onyx has much lower and much stable (higher percentiles similar to lower percentiles) latency than the other techniques.

In the second result, we have CloudEx, a previous system and Onyx. We are plotting delivery window size i.e., for each message what is the max difference in reception time by any two receivers. We target a delivery window size of <1 microsecond (which is shown as 0). 

In the third result, we have LOQ vs FIFO order matching rate. we previously saw similar graph for the entire order submission service vs a previous system cloudex. but this graph just shows the benefit of LOQ and FIFO. The graph shpws how LOQ is able to effectively handle bursts (shaded regions). 


Slide: Concluding Remarks
========

- Cloud finanicial exchanges can be realized by developing new primitives that achieve fairness and achive high performance, and it can be done so without specialized infrastructure that on-prem exchange/colocation facilities use

- but it may require relazing some of the fairness guarantees e.g., all of Ony's guarantees are at the level of microsecond-timescale, while some on-prem exchange go at sub-microsecond timescale. 

- Cloud financial exchanges present a new operating point in the cost-performance curve. 

- they are better at performance and fairness compared to web-apis where market data is dissemnated using web-apis and orders also placed without over the public internet. 

- they are better at scalability and cost compared to on-prem exchanges which use colocation facilities. 

- and accordinly cloud exchanges are worse at some axis as well as shown in this figure. 

=======
QA:


- does cloud really lead to cost savings?
--- Ans: it touches a sophisticated point about cloud migration and cloud repatriation. Cloud reptration measn moving away from cloud which some companies have realized that it saves costs. However, cloud repatriaion applies to cloud-scale clients which can benefit themsleves from economy of scale by owning infrastructure e.g., dropbox or snowflake. other smaller clients can still benefit from public cloud in terms of costs. 


- proof of LOQ?
--- Ans: while the paper explains the intuituon and argues why LOQ preserves fairness a formal proof has not been done yet. 



- LOQ impacting dynamics of market in a way that it changes the behavior of traders?
--- Ans: a valid concern but we have not explored it. 


- Why would a trader not lie about following hold-and-release or attaching generation timestamps?

-- a trader may indeed try to manipulate whatever is in their control to win the competition. in the talk, we did not touch the deployment model, but in the paper we explain that the all VMs are under the exchange's control and only trading code from the traders is loaded in the VMs. The code for following exchange's protocols (hold and release and attaching generation timestamps) is also loaded by the exchange and it is not provided by the trader (as we cannot trust that). There is more discussion about security implications in the appendix of the paper. 